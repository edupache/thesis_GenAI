{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commands to run in the terminal to install the required packages\n",
    "\"\"\" \n",
    "conda create -n ollama python=3.10.9\n",
    "conda activate ollama\n",
    "pip install -r requirements.txt\n",
    "\"\"\"\n",
    "# ! pip install ollama "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import ollama\n",
    "import chromadb\n",
    "#from langchain_chroma import Chroma\n",
    "#from langchain_community.embeddings import OllamaEmbeddings\n",
    "#import chromadb.utils.embedding_functions as embedding_functions\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# Constants Definition\n",
    "PERSIST_DIRECTORY = \"C:\\\\Users\\\\eduar\\\\Documents\\\\Master_Thesis\\\\GenAI_Thesis_Beekeeper\\\\data\\\\datasets_db\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see how many model are included\n",
    "import ollama\n",
    "\n",
    "# Pull mistral model\n",
    "# ollama.pull(\"mistral\")\n",
    "# ollama.show(\"mistral\")\n",
    "ollama.list()['models']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with \"ollama2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model to llama2 and chat with it\n",
    "stream = ollama.chat(\n",
    "    model='mistral',\n",
    "    messages=[{'role': 'user', 'content': 'What is Flagger Force?'}],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "  print(chunk['message']['content'], end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Context to the Model Response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import bs4\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "loader = WebBaseLoader(    \n",
    "    web_paths=(\"https://flaggerforce.com/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"page-content\", \"page-body\", \"page-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "docs\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import users file\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "#path = \"/home/ubuntu/thesis_GenAI/data/production_datasets/cleaned_datasets\"\n",
    "path = \"C:\\\\Users\\\\eduar\\\\Documents\\\\Master_Thesis\\\\GenAI_Thesis_Beekeeper\\\\data\\\\production_datasets\\\\cleaned_datasets\"\n",
    "file_name = \"users_cleaned.csv\"\n",
    "file_path = os.path.join(path, file_name)\n",
    "df_users = pd.read_csv(file_path)\n",
    "df_users.head()\n",
    "\n",
    "# df_users.iloc[:20,:].to_csv(\"C:\\\\Users\\\\eduar\\\\Documents\\\\Master_Thesis\\\\GenAI_Thesis_Beekeeper\\\\data\\\\production_datasets\\\\cleaned_datasets\\\\users_to_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open (\"C:\\\\Users\\\\eduar\\\\Documents\\\\Master_Thesis\\\\GenAI_Thesis_Beekeeper\\\\data\\\\production_datasets\\\\cleaned_datasets\\\\users_to_test.csv\") as file:\n",
    "    lines = csv.reader(file)\n",
    "    \n",
    "    # Initialize list variables\n",
    "    documents = []\n",
    "    metadatas = []\n",
    "    ids = []\n",
    "    id = 1\n",
    "    \n",
    "    # Iterate through csv by row\n",
    "    for i, line in enumerate(lines):\n",
    "        if i==0:\n",
    "            continue\n",
    "        \n",
    "        documents.append(line[3])   # Adding the username as main information\n",
    "        metadatas.append({'first_login':line[1], 'id':line[4], 'position':line[17], 'department':line[15], 'suspended':line[2], \n",
    "                          'last_skipped_contact_details':line[5], 'state':line[6], 'role':line[7], 'last_login':line[8],'updated':line[9],\n",
    "                          'used':line[10], 'display_name_extension':line[11], 'suspended_at':line[12], 'language':line[13], 'created':line[14], 'location':line[16]})\n",
    "        ids.append(str(id))\n",
    "        id+=1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "chroma_client = chromadb.Client(ALLOW_RESET=TRUE)\n",
    "collection_users = chroma_client.create_collection(name=\"users_collection\")\n",
    "# collection_posts = chroma_client.create_collection(name=\"posts_collection\")\n",
    "# collection_comments = chroma_client.create_collection(name=\"comments_collection\")\n",
    "# collection_streams = chroma_client.create_collection(name=\"streams_collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chroma_client.delete_collection(name=\"users_collection\")\n",
    "#vectorstore.delete_collection()\n",
    "\n",
    "chroma_client.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Chroma uses any Sentence Transformers (https://www.sbert.net/) model to create embeddings. Default model is \"all-MiniLM-L6-v2\",\n",
    "other model are available: https://www.sbert.net/docs/pretrained_models.html  \n",
    "sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"all-MiniLM-L6-v2\")\n",
    "\"\"\"\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "PERSIST_DIRECTORY = \"C:\\\\Users\\\\eduar\\\\Documents\\\\Master_Thesis\\\\GenAI_Thesis_Beekeeper\\\\data\\\\vectordb\"\n",
    "embedding = OllamaEmbeddings(model=\"mistral\")\n",
    "vectorstore = Chroma.from_documents(    \n",
    "    documents=splits, \n",
    "    embedding=embedding,\n",
    "    #metadatas=metadatas,\n",
    "    ids=ids,\n",
    "    persist_directory=PERSIST_DIRECTORY\n",
    "    )\n",
    "\n",
    "\n",
    "# collection_users.add(\n",
    "#     #embeddings=[[1.2, 2.3, 4.5], [6.7, 8.2, 9.2]],\n",
    "#     documents=documents,\n",
    "#     embeddings=embeddings,\n",
    "#     metadatas=metadatas,\n",
    "#     ids=ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = collection_users.query(\n",
    "    query_texts=[\"Aaron\"],\n",
    "    n_results=2,\n",
    "    include=['distances','documents']\n",
    ")\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Comments to Chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stream_name</th>\n",
       "      <th>created_date</th>\n",
       "      <th>post_id</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>like_count</th>\n",
       "      <th>report_count</th>\n",
       "      <th>username</th>\n",
       "      <th>author_user_id</th>\n",
       "      <th>author_position</th>\n",
       "      <th>author_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26493</th>\n",
       "      <td>Jobs</td>\n",
       "      <td>2023-09-22 05:20:45</td>\n",
       "      <td>7050379</td>\n",
       "      <td>6232334</td>\n",
       "      <td>what happened?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Vera_Rivera</td>\n",
       "      <td>46a5d6ae-52ae-4b13-8add-65e46ed7d98a</td>\n",
       "      <td>Crew Member</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33229</th>\n",
       "      <td>Jobs</td>\n",
       "      <td>2023-08-13 12:20:27</td>\n",
       "      <td>6915828</td>\n",
       "      <td>6078965</td>\n",
       "      <td>I will go</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Ira_Crusan</td>\n",
       "      <td>0826bcb2-a311-443a-a3a9-d62c169388a7</td>\n",
       "      <td>Crew Leader</td>\n",
       "      <td>suspended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92731</th>\n",
       "      <td>Safety &amp; Operations</td>\n",
       "      <td>2022-09-13 17:59:39</td>\n",
       "      <td>5751264</td>\n",
       "      <td>4800765</td>\n",
       "      <td>@Amber_Davenport congratulations! Thank you fo...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Scott_Richwine</td>\n",
       "      <td>2aac35e8-f970-4434-9b46-56274384cf89</td>\n",
       "      <td>Safety Manager</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41111</th>\n",
       "      <td>Water Break</td>\n",
       "      <td>2023-06-29 23:40:53</td>\n",
       "      <td>6753487</td>\n",
       "      <td>5912239</td>\n",
       "      <td>DOSE ANYONE KNOW WERE TO GET THIS</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>David_Unkelbach</td>\n",
       "      <td>47aa9966-5f04-47c3-b90b-753aa742753f</td>\n",
       "      <td>Advanced Crew Leader</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73580</th>\n",
       "      <td>Jobs</td>\n",
       "      <td>2022-12-24 14:04:15</td>\n",
       "      <td>6106979</td>\n",
       "      <td>5194273</td>\n",
       "      <td>Ty</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Charles_Yates</td>\n",
       "      <td>03bea137-fadd-40a5-af8c-52c93922da53</td>\n",
       "      <td>Advanced Crew Leader</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               stream_name         created_date  post_id  comment_id  \\\n",
       "26493                 Jobs  2023-09-22 05:20:45  7050379     6232334   \n",
       "33229                 Jobs  2023-08-13 12:20:27  6915828     6078965   \n",
       "92731  Safety & Operations  2022-09-13 17:59:39  5751264     4800765   \n",
       "41111          Water Break  2023-06-29 23:40:53  6753487     5912239   \n",
       "73580                 Jobs  2022-12-24 14:04:15  6106979     5194273   \n",
       "\n",
       "                                            comment_text  like_count  \\\n",
       "26493                                     what happened?           0   \n",
       "33229                                          I will go           0   \n",
       "92731  @Amber_Davenport congratulations! Thank you fo...           1   \n",
       "41111                  DOSE ANYONE KNOW WERE TO GET THIS           0   \n",
       "73580                                                 Ty           1   \n",
       "\n",
       "       report_count         username                        author_user_id  \\\n",
       "26493             0      Vera_Rivera  46a5d6ae-52ae-4b13-8add-65e46ed7d98a   \n",
       "33229             0       Ira_Crusan  0826bcb2-a311-443a-a3a9-d62c169388a7   \n",
       "92731             0   Scott_Richwine  2aac35e8-f970-4434-9b46-56274384cf89   \n",
       "41111             0  David_Unkelbach  47aa9966-5f04-47c3-b90b-753aa742753f   \n",
       "73580             0    Charles_Yates  03bea137-fadd-40a5-af8c-52c93922da53   \n",
       "\n",
       "            author_position author_status  \n",
       "26493           Crew Member        active  \n",
       "33229           Crew Leader     suspended  \n",
       "92731        Safety Manager        active  \n",
       "41111  Advanced Crew Leader        active  \n",
       "73580  Advanced Crew Leader        active  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import users file\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "#path = \"/home/ubuntu/thesis_GenAI/data/production_datasets/cleaned_datasets\"\n",
    "path = \"C:\\\\Users\\\\eduar\\\\Documents\\\\Master_Thesis\\\\GenAI_Thesis_Beekeeper\\\\data\\\\production_datasets\\\\cleaned_datasets\"\n",
    "file_name = \"comments_cleaned.csv\"\n",
    "file_path = os.path.join(path, file_name)\n",
    "df_comments = pd.read_csv(file_path)\n",
    "\n",
    "# Create a sample of 50,000 comments to test\n",
    "df_comments_to_test = df_comments.sample(300)\n",
    "# Print a sample\n",
    "df_comments.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>author_position</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Advanced Crew Leader</th>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Crew Member</th>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Crew Leader</th>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Weekend Dispatch</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lead Instructor</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Area Supervisor</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Field Trainer 1</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Warehouse Coordinator</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Field Trainer 2</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Executive Assistant</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Internal Communications Manager</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Field Manager</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Safety Professional</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Internal Communications Coordinator</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Employee Services Supervisor</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Safety Advocate</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Employee Services Specialist</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Employee Services and Field Recruiting Manager</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Client Services Manager</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Training Supervisor</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Business Development Manager</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Northern Safety Supervisor</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Southeastern Territory Manager</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Employee Services</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Claims Coordinator</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Brand and Marketing Specialist</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Claims Manager</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                count\n",
       "author_position                                      \n",
       "Advanced Crew Leader                              207\n",
       "Crew Member                                       109\n",
       "Crew Leader                                        90\n",
       "Weekend Dispatch                                   10\n",
       "Lead Instructor                                    10\n",
       "Area Supervisor                                     8\n",
       "Field Trainer 1                                     6\n",
       "Warehouse Coordinator                               5\n",
       "Field Trainer 2                                     4\n",
       "Executive Assistant                                 4\n",
       "Internal Communications Manager                     4\n",
       "Field Manager                                       3\n",
       "Safety Professional                                 3\n",
       "Internal Communications Coordinator                 3\n",
       "Employee Services Supervisor                        3\n",
       "Safety Advocate                                     2\n",
       "Employee Services Specialist                        2\n",
       "Employee Services and Field Recruiting Manager      2\n",
       "Client Services Manager                             1\n",
       "Training Supervisor                                 1\n",
       "Business Development Manager                        1\n",
       "Northern Safety Supervisor                          1\n",
       "Southeastern Territory Manager                      1\n",
       "Employee Services                                   1\n",
       "Claims Coordinator                                  1\n",
       "Brand and Marketing Specialist                      1\n",
       "Claims Manager                                      1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count how times a user has commented\n",
    "df_comments_to_test['author_position'].value_counts().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stream_name</th>\n",
       "      <th>created_date</th>\n",
       "      <th>post_id</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>like_count</th>\n",
       "      <th>report_count</th>\n",
       "      <th>username</th>\n",
       "      <th>author_user_id</th>\n",
       "      <th>author_position</th>\n",
       "      <th>author_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Water Break</td>\n",
       "      <td>2024-03-05 15:15:55</td>\n",
       "      <td>7612625</td>\n",
       "      <td>6866587</td>\n",
       "      <td>Congratulations and a great addition to the te...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>FlaggerForce</td>\n",
       "      <td>b701ab9f-563a-4425-a389-aff803a8da58</td>\n",
       "      <td>NaN</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Water Break</td>\n",
       "      <td>2024-03-05 15:10:47</td>\n",
       "      <td>7612663</td>\n",
       "      <td>6866567</td>\n",
       "      <td>Hello team, remember to have safety discussion...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>FlaggerForce</td>\n",
       "      <td>b701ab9f-563a-4425-a389-aff803a8da58</td>\n",
       "      <td>NaN</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Water Break</td>\n",
       "      <td>2024-03-05 15:08:29</td>\n",
       "      <td>7613861</td>\n",
       "      <td>6866561</td>\n",
       "      <td>@Larry_Broderick, both our moderation team and...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>FlaggerForce</td>\n",
       "      <td>b701ab9f-563a-4425-a389-aff803a8da58</td>\n",
       "      <td>NaN</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Safety &amp; Operations</td>\n",
       "      <td>2024-03-05 15:04:42</td>\n",
       "      <td>7613237</td>\n",
       "      <td>6866545</td>\n",
       "      <td>@William_Pulliam, we appreciate you being prou...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>FlaggerForce</td>\n",
       "      <td>b701ab9f-563a-4425-a389-aff803a8da58</td>\n",
       "      <td>NaN</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Safety &amp; Operations</td>\n",
       "      <td>2024-03-05 15:02:47</td>\n",
       "      <td>7613255</td>\n",
       "      <td>6866539</td>\n",
       "      <td>We appreciate you supporting @Dennis_Cumbie, @...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>FlaggerForce</td>\n",
       "      <td>b701ab9f-563a-4425-a389-aff803a8da58</td>\n",
       "      <td>NaN</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139662</th>\n",
       "      <td>Safety &amp; Operations</td>\n",
       "      <td>2022-01-03 21:28:23</td>\n",
       "      <td>4928202</td>\n",
       "      <td>3861062</td>\n",
       "      <td>Big kudos to @Karen_Skwara.She always goes abo...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>FlaggerForce</td>\n",
       "      <td>b701ab9f-563a-4425-a389-aff803a8da58</td>\n",
       "      <td>NaN</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139664</th>\n",
       "      <td>Safety &amp; Operations</td>\n",
       "      <td>2022-01-03 21:25:24</td>\n",
       "      <td>4929313</td>\n",
       "      <td>3861053</td>\n",
       "      <td>First snow of the season @April_Long?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>FlaggerForce</td>\n",
       "      <td>b701ab9f-563a-4425-a389-aff803a8da58</td>\n",
       "      <td>NaN</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139665</th>\n",
       "      <td>Safety &amp; Operations</td>\n",
       "      <td>2022-01-03 21:24:39</td>\n",
       "      <td>4929597</td>\n",
       "      <td>3861047</td>\n",
       "      <td>Excellent safety message @Samuel_Wirtz. Clear ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>FlaggerForce</td>\n",
       "      <td>b701ab9f-563a-4425-a389-aff803a8da58</td>\n",
       "      <td>NaN</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139667</th>\n",
       "      <td>Safety &amp; Operations</td>\n",
       "      <td>2022-01-03 21:22:24</td>\n",
       "      <td>4930738</td>\n",
       "      <td>3861033</td>\n",
       "      <td>We are very grateful for this good news. Stay ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>FlaggerForce</td>\n",
       "      <td>b701ab9f-563a-4425-a389-aff803a8da58</td>\n",
       "      <td>NaN</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139668</th>\n",
       "      <td>Safety &amp; Operations</td>\n",
       "      <td>2022-01-03 21:21:43</td>\n",
       "      <td>4930751</td>\n",
       "      <td>3861029</td>\n",
       "      <td>Yes, perfect! Keep 'em coming!</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>FlaggerForce</td>\n",
       "      <td>b701ab9f-563a-4425-a389-aff803a8da58</td>\n",
       "      <td>NaN</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3270 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                stream_name         created_date  post_id  comment_id  \\\n",
       "0               Water Break  2024-03-05 15:15:55  7612625     6866587   \n",
       "1               Water Break  2024-03-05 15:10:47  7612663     6866567   \n",
       "2               Water Break  2024-03-05 15:08:29  7613861     6866561   \n",
       "3       Safety & Operations  2024-03-05 15:04:42  7613237     6866545   \n",
       "4       Safety & Operations  2024-03-05 15:02:47  7613255     6866539   \n",
       "...                     ...                  ...      ...         ...   \n",
       "139662  Safety & Operations  2022-01-03 21:28:23  4928202     3861062   \n",
       "139664  Safety & Operations  2022-01-03 21:25:24  4929313     3861053   \n",
       "139665  Safety & Operations  2022-01-03 21:24:39  4929597     3861047   \n",
       "139667  Safety & Operations  2022-01-03 21:22:24  4930738     3861033   \n",
       "139668  Safety & Operations  2022-01-03 21:21:43  4930751     3861029   \n",
       "\n",
       "                                             comment_text  like_count  \\\n",
       "0       Congratulations and a great addition to the te...           0   \n",
       "1       Hello team, remember to have safety discussion...           0   \n",
       "2       @Larry_Broderick, both our moderation team and...           0   \n",
       "3       @William_Pulliam, we appreciate you being prou...           0   \n",
       "4       We appreciate you supporting @Dennis_Cumbie, @...           0   \n",
       "...                                                   ...         ...   \n",
       "139662  Big kudos to @Karen_Skwara.She always goes abo...           2   \n",
       "139664              First snow of the season @April_Long?           0   \n",
       "139665  Excellent safety message @Samuel_Wirtz. Clear ...           1   \n",
       "139667  We are very grateful for this good news. Stay ...           0   \n",
       "139668                     Yes, perfect! Keep 'em coming!           0   \n",
       "\n",
       "        report_count      username                        author_user_id  \\\n",
       "0                  0  FlaggerForce  b701ab9f-563a-4425-a389-aff803a8da58   \n",
       "1                  0  FlaggerForce  b701ab9f-563a-4425-a389-aff803a8da58   \n",
       "2                  0  FlaggerForce  b701ab9f-563a-4425-a389-aff803a8da58   \n",
       "3                  0  FlaggerForce  b701ab9f-563a-4425-a389-aff803a8da58   \n",
       "4                  0  FlaggerForce  b701ab9f-563a-4425-a389-aff803a8da58   \n",
       "...              ...           ...                                   ...   \n",
       "139662             0  FlaggerForce  b701ab9f-563a-4425-a389-aff803a8da58   \n",
       "139664             0  FlaggerForce  b701ab9f-563a-4425-a389-aff803a8da58   \n",
       "139665             0  FlaggerForce  b701ab9f-563a-4425-a389-aff803a8da58   \n",
       "139667             0  FlaggerForce  b701ab9f-563a-4425-a389-aff803a8da58   \n",
       "139668             0  FlaggerForce  b701ab9f-563a-4425-a389-aff803a8da58   \n",
       "\n",
       "       author_position author_status  \n",
       "0                  NaN        active  \n",
       "1                  NaN        active  \n",
       "2                  NaN        active  \n",
       "3                  NaN        active  \n",
       "4                  NaN        active  \n",
       "...                ...           ...  \n",
       "139662             NaN        active  \n",
       "139664             NaN        active  \n",
       "139665             NaN        active  \n",
       "139667             NaN        active  \n",
       "139668             NaN        active  \n",
       "\n",
       "[3270 rows x 11 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comments[df_comments['username']=='FlaggerForce']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['@Harold_Friscia lol. In my backyard lol.',\n",
       " 'Congratulations Austin!!',\n",
       " '@Duwan_Maye oh they playing with you bruh lol',\n",
       " '@Julie_Snedeker when I left from up there it was raining ice, by the time I got back in MD it was raining.',\n",
       " 'Me for Baltimore location @weekend_dispatch @weekend_dispatch',\n",
       " 'Available',\n",
       " \"I called your stand in for you. She got it canceled, but the TCI training wasn't  canceled. That's why I got the 2 points. I knew you were away doing training, etc. That's why I called your backup. I met her today also. Don't  remember her name but she came from Georgia, I believe.\",\n",
       " 'My condolences to his family 🕊️',\n",
       " 'With you',\n",
       " 'Welcome to Flagger Force!',\n",
       " 'Okay, cat lady! 🤣',\n",
       " 'Me too',\n",
       " 'should be interesting',\n",
       " 'I can go',\n",
       " '@Raymond_Moody and @Edwin_SkinnerJr  you have been scheduled.  Thank you!!!!',\n",
       " 'i am',\n",
       " 'Congrts u two well done',\n",
       " '@jason_miller  no problem. I wasn’t sure where in western you were.',\n",
       " '@James_WootenIII and me',\n",
       " \"Also for number 1, it depends if its only a short time , We just close it and don't call anyone, but where we close it, people are able to go down a side street safely and never into a parking lot or the middle of the road to turn around\",\n",
       " 'Me',\n",
       " \"How long have you been here? Have you gone to CMDT yet? It all happens in steps. After I think 3 weeks you go to CMDT for I think it was one day, then in another week if you trying to get in a truck you go to BWZ which is two days, then you go in the field with FT1 for 3 or how ever many days it takes for you to pass and be comfortable enough. Usually your supervisor comes around and asks if you want to be in the class. If you have been through CMDT already though, it would never hurt to reach out to your AS. After all, isn't it best to show your interest? In that case, you're not considered being a pest. In fact, they like when we show initiative...😉\",\n",
       " 'Me',\n",
       " '@itz_fleet can you assist please?',\n",
       " '👋🏾',\n",
       " '@Marie_Wanamaker , I appreciate all of the love and support that you’ve shown me from day one💯💪🏾🔥! Good looking on the shoutout \\U0001f979🤣😂🤣',\n",
       " 'Me',\n",
       " 'I’ll go',\n",
       " 'Available',\n",
       " '545023',\n",
       " 'Do we get anything extra for having said equipment',\n",
       " 'Truth',\n",
       " 'I don’t think Japan is safe to flag in too much going on',\n",
       " 'Anytime champ',\n",
       " 'They have those in fl as well but only in the back roads',\n",
       " 'Covid is over rated and the mask is useless',\n",
       " '😂all these dog puns are giving me the scurvy yarrr',\n",
       " \"Then why is when I call in to flagger force, certain people say you have to wait until dispatch, if you wait until people are dispatched and you aren't on the dispatch, isn't that an issue, shouldn't you get put right on it?\",\n",
       " 'Congradulations',\n",
       " '@itz_fleet can you please review this post and provide further insight.',\n",
       " 'When u first walk in on Feb 1st make sure u say HOME SWEET HOME ! & CONGRATS 👏',\n",
       " 'Me',\n",
       " '@client_services_transition',\n",
       " 'This order has been filled. We are still looking for 1 CL from Western who can deliver a PLT and drop it off at the site',\n",
       " 'Thanks Robert do you know what option to press ?',\n",
       " \"I'll go\",\n",
       " 'The amount of ice patches I’ve seen insane. Stay safe everyone.',\n",
       " \"Plus no one drives the speed limit on that road. That's going towards the Gordon Nagle trail to Pottsville and Minersville\",\n",
       " 'Grace take a deep breath and pray, God will not put on you no more than you can handle, everything will be OK',\n",
       " \"Woah.😯😳🤯....that is big job 👏👏👏👏 hats off to y'all's great work\",\n",
       " \"Damn that's messed up\",\n",
       " 'Avaliable',\n",
       " 'Me',\n",
       " 'Thank u Jennifer',\n",
       " 'Who? Maybe they’re on vacation',\n",
       " \"Let's do it @Taylor_Schalk\",\n",
       " 'Me',\n",
       " 'You got that right ✅️',\n",
       " 'Yes you can be anything you want even a comedian',\n",
       " \"Shonda you are welcome and thank you for being a great leader and teacher today I've learned some things from you and will carry that with me and thanks to the rest of the crew for being a good supporting crew\",\n",
       " 'We look forward to seeing your pictures all week!',\n",
       " '😂😂😂😂',\n",
       " \"It'd be closer to 3 hrs by the time Id stop in New Ken for the PLT and then get out there, sadly\",\n",
       " '@Samuel_Wirtz',\n",
       " 'Ray I wish I could but you just got off work 😟',\n",
       " 'Looks good, what are the 2 cones for that look out of place? Educate us please!',\n",
       " '🙏🏾🙏🏾🙏🏾🙏🏾🙏🏾🙏🏾🙏🏾',\n",
       " 'available',\n",
       " 'Available',\n",
       " '☝🏼☝🏼☝🏼 what @Toby_Germinario  said',\n",
       " 'Congratulations on your monumental accomplishment.',\n",
       " 'Does this actually work when you post this? Serious question, I’m not trying to make fun. And this question is for everyone. Do they actually reach out and give work to people who post “I’m available”',\n",
       " 'CONGRATS, Seth.',\n",
       " '  me @@weekend_dispatch',\n",
       " 'Great job',\n",
       " \"I look forward to seeing you all in the field.  Sorry I couldn't be there for you guys to put a face to a name but in due time I will meet each of you!\",\n",
       " 'I’m available !!',\n",
       " 'But she is on call for wpa this weekend',\n",
       " 'Available for tailgating \\nEquipment runs \\nIndoor pool parties \\nEmergency orders \\nEven bowling 🎳 \\nReplacements \\nNight work \\nHighway jobs \\nPainting \\nLandscaping \\nRemolding \\nNew construction',\n",
       " 'I’ll go',\n",
       " 'Hubert radcliff',\n",
       " \"Can't get a hold of them\",\n",
       " 'Available',\n",
       " 'Sorry for your loss',\n",
       " 'available',\n",
       " 'Way to girl',\n",
       " '@Collette_Monaghan, I can not agree with you more! Marty Rocks!',\n",
       " 'You got that right',\n",
       " 'Im already at perry hall service center i can stay',\n",
       " 'Hey @Adrienne_Long',\n",
       " 'Welcome Aboard!🎉',\n",
       " 'Me please',\n",
       " 'I live in Point Marion not sure what all is close to that.',\n",
       " \"It's only a hour and 22 to the job site from my place\",\n",
       " 'Thank you so much Paul!!!!!!!!',\n",
       " 'my order from lastnight had the ap on there, I just got off this morning.',\n",
       " 'Awesome!',\n",
       " 'Available',\n",
       " 'Welcome!',\n",
       " 'Thank you !😊',\n",
       " 'How long',\n",
       " \"Hey I'm available for work this weekend just let me if you still looking for available CM WEEKEND DISPATCH\",\n",
       " 'Awesome advice... thank you for the reminder. \\n\\nTo insure i have properly secured the AP. I always jack it back up to make sure my truck lifts after i have locked the ball in place. I triple check the legs as i have had legs drop randomly merging onto the highway.',\n",
       " \"It didn't show a dispatch, I don't know if um on first call or is the app not working?\",\n",
       " 'Will be done at 8 30 am will tell client to in morning',\n",
       " 'Thank you katie',\n",
       " 'Good luck to you man!!',\n",
       " 'Lemme get a plate of that goulash',\n",
       " 'Me\\n',\n",
       " \"One where's the signs road work ahead, one lane, flagger? Where's the cone's tapper's buffer dead zone work space oh and the other flagger...????? WOW 😳😳 wrong\",\n",
       " 'U need help',\n",
       " 'Great to see an in-progress session of some \"Active Monitoring\". How was the road closure?',\n",
       " 'They have their specials',\n",
       " 'My lead last week stayed by the truck and played on his phone all day, every day.',\n",
       " \"If it was an emergency order, that may have been why\\n\\nI called about the one in Somerset a couple weeks ago, and they said no, because they were trying to find people closer.\\n\\n\\nA wise woman did my orientation, I think her name was @Jennifer_Hartley .\\n\\nShe made sure to let us know to bug OSC, there'll eventually be something that pops up\",\n",
       " '@Diane_Taylor , I believe you can go into Kronos and complete a form allowing you donate.  I will send you a private message.',\n",
       " 'Did you work today',\n",
       " 'I’ll go if it’s within 2hrs of me',\n",
       " 'Get me in a truck',\n",
       " 'Do you have orders now',\n",
       " 'I can also pick someone up if need be',\n",
       " 'It’s cool payroll doesn’t go in til Monday it aint that serious just give him a call or text if it ain’t in by the time u wake up Monday morning then call OSC',\n",
       " 'Just buy a bumper sticker.. People will custom make them for you. Etsy.',\n",
       " 'Payday  is Fridays',\n",
       " 'Awesome',\n",
       " 'Where are you?',\n",
       " 'Ready to go',\n",
       " 'Is it in Johnstown',\n",
       " 'I will',\n",
       " '@Jacob_Hardy . The \"rentals\" have FL tags',\n",
       " \"I'm available to work\",\n",
       " 'Me',\n",
       " 'Congratulations everyone!',\n",
       " '@weekend_dispatch available',\n",
       " 'Ty very much everyone',\n",
       " 'I called an they said it wouldn’t be a problem if I had to get another one bc I’m not in yet anyway I don’t mind I like driving I thought I was doing what I was told sorry if it upset u I have all my handbooks papers notes etc I have no reason to try an mess anything up i apologize for making a problem if I did',\n",
       " 'Man let me get that money sir please get you some rest',\n",
       " '?',\n",
       " 'Have to be certified, MD VA Atl NC SC and FL',\n",
       " 'Awesome job @tamara_palmer1 and @Tyler_Orazio!! Excellence in action!!🎉👷🏼\\u200d♀️🛑',\n",
       " 'i can go',\n",
       " 'aw shux, j can go up there',\n",
       " 'The system build for u fail …   🧘🏾\\u200d♂️',\n",
       " \"Happy Valentine's Day\",\n",
       " \"I'll go\",\n",
       " \"Notorious RBG...She is a legend. She's an inspiration for her intelligence, hard work ethic, and objectivity.\",\n",
       " 'I am available for the Scranton job or honesdale\\n',\n",
       " 'Available!!',\n",
       " 'i feel like all they need to start doing is put people on that WANT and NEED to work and either the people that don’t want to fire or just don’t put them on jobs, they’ll take the hint when they aren’t getting money',\n",
       " '👍🙏congratulations',\n",
       " 'Michael McPherson cool person if you need some help I will go',\n",
       " \"I'm absolutely positive the two of you held down the fort just fine! Thank you for sharing your experience with us! Awesome picture too!\",\n",
       " \"Anton how's you mom\",\n",
       " 'On itz( in the zone)  go to home page, bottom right more tab check everything out',\n",
       " 'Available',\n",
       " 'I can go if I can keep my morning job. I live 2 mins from my morning job @weekend_dispatch ',\n",
       " 'Available',\n",
       " 'Congrats on your promotion.  Typically, this occurs that Monday after you sign the paperwork, which is today.',\n",
       " 'Congratulations, @Brandon_Berkebile!  Keep up the great work.',\n",
       " 'Gonna miss working with you.   Stay Safe',\n",
       " 'Available and willing  to travel',\n",
       " \"Well if you open up a can of worms this is what you get @John_Ware . I'm just saying\",\n",
       " 'Yeah man idk how I’m putting gas in my car for work next week lol',\n",
       " 'Me',\n",
       " '@Sean_Vazquez @Sean_Vazquez',\n",
       " \"Love the Archie's\",\n",
       " 'Charles have a great day',\n",
       " \"That's ok thanks\",\n",
       " 'Congratulations!   Awesome news!!!  🙌🎉',\n",
       " 'Wow! Very insightful! Thank you for posting @ross_miller!',\n",
       " '@David_Wildey this looks great to me',\n",
       " 'Y’all got these big trucks and no 4wheel drive that’s crazy and u tow equipment with them pour transmission',\n",
       " '@Latece_Greer yes this has been filled',\n",
       " 'I’m certified for both I got based outta central pa @weekend_dispatch',\n",
       " \"@Annastina_VonSchmeling living in Dallas.... I'm sure you're used to that kind of stuff. Native to that area? I grew up in Tunkhannock so... Yea I guess I can say I'm a country boy at heart\",\n",
       " 'Taking picture while controlling a vehicle.',\n",
       " '@Renard_Patterson',\n",
       " 'Me',\n",
       " 'Available',\n",
       " '@Toby_Germinario thanks bro appreciate you',\n",
       " '@thomas_mcginnis  @Shonda_Smith thank you guys',\n",
       " 'I can go if needed',\n",
       " \"@Jonathan_Muha... Good luck on your next adventure. It's been a pleasure working beside you all these years. You've taught me quite a bit and I hope I've helped you along the way as well. You know how to reach me if you need anything. You will be missed. Be safe.\",\n",
       " 'Facts',\n",
       " 'I can grab one on the way if Need to Thanx!',\n",
       " \"I'm ready\",\n",
       " 'Send me all weekend and all week 💪💪',\n",
       " \"I'm 15 minutes away\",\n",
       " 'Congratulations Sir',\n",
       " 'Acl will travel',\n",
       " 'Micheal palmer',\n",
       " \"I don't know who Thurmont is but that is sad. If society wants a better situation for everyone  attitudes of squashing people keeping them down then blaming for ills of the world needs to stop. Try this equation  .. = - ÷ /100%?\",\n",
       " 'Welcome to Flagger Force',\n",
       " 'Thanks @Daniel_Duncan & the same thing goes for all of you guys great worker',\n",
       " '@Grace_Billing what kinda car is it',\n",
       " 'Same here.\\nWe just went a full weekend of completely locked out of the app until Monday to get our password reset.\\nNow this.\\nCan we please go back to the old app????',\n",
       " \"It's coming to pa\",\n",
       " 'Welcome Back',\n",
       " \"Hi @Danielle_Schall ,   I'm sorry to hear about your family member.  I will call you.\",\n",
       " 'Prayers to you and your family @Charles_Northrop.',\n",
       " 'Where is that at',\n",
       " '@Mark_Matthews',\n",
       " 'Congratulations to all of you!! 🎉',\n",
       " 'Stay safe Miss Willis, it was a pleasure working with you',\n",
       " 'If they cancel my order which they probably will can u add me',\n",
       " 'Available',\n",
       " 'It was great seeing you the other day while doing side visits with @reed_sheaffer! Keep up the good work!',\n",
       " 'email employee services and I believe the limit is now $30 a month',\n",
       " 'He is a hard worker for sure, had him on a tricky night job in gettysburg for two days a couple weeks ago and he was nothing but professional and always asked the right questions, great job @Brandon_Bahrenburg',\n",
       " \"Yeah that's crazy too that you had to bcuz when YOU have to call and get it fixed that it's like a big F U\",\n",
       " 'Happy Birthday, @Laura_Land!',\n",
       " '@Marcus_Almeida , they have Johnstown Verizon, i called right at 530, and was 9th in queue',\n",
       " 'I appreciate the help',\n",
       " 'Available',\n",
       " '@Matthew_Benzing  that is the number of the tma I got lol',\n",
       " 'But we gone now I’m abou to pull off',\n",
       " 'Me ill go',\n",
       " \"@Toby_Germinario guess you ain't #1 no more!\",\n",
       " 'Cap I understand what you saying buuutttt some of these cls and acls don’t know what they doing I had a job where the lead kept asking me how to set up kept leaving the job site we got no break and we didn’t have to flag it could’ve been shoulder work frfr some of these people need to be demoted or sum idk',\n",
       " 'Is noone going to bring up his vest is not ours and there no sleeves and y no paddle',\n",
       " '**This has been filled**',\n",
       " 'If you call OSC they can tell you.. or you can look up what dates you got pointed for through ITZ',\n",
       " 'Me too',\n",
       " 'Me Russell St.',\n",
       " 'Available',\n",
       " 'I never get my orders for the whole week, been day to day.',\n",
       " '🤣😂🤣',\n",
       " 'Sorry never mind...way further than I thougjt....lol...wmh',\n",
       " 'Who the lead',\n",
       " '@Waylon_Curtis for your AP set up for today.',\n",
       " 'Mellissa Brown Thank you for your input. I believe yourmisunderstanding the post. I’m not looking for the FS position at all, been there done that during the pandemic. I also know that the position is being phased out. The post was an attempt to get the job description and duty description in order to update my resume accurately. Somehow that has been misconstrued by some people for reasons I won’t get into.',\n",
       " 'Happy birthday 🎂',\n",
       " \"I'm available\",\n",
       " 'i can go too, work and witness',\n",
       " '@Tomika_Williams',\n",
       " \"I'm available\",\n",
       " \"Hi, @Brionna_Gillison. We're sorry that you haven't been receiving as many hours as expected. Please make sure you are available to receive calls at 5:30 p.m. for jobs. You can also check the Jobs stream for open jobs in your area and comment on the post with your information (location, certification). Additionally, you can try to call your area supervisor to see if there are any jobs that they can send you to. We hope this helps. Have a safe day!\",\n",
       " 'What’s the assignment',\n",
       " 'I love seeing posts like this!',\n",
       " 'I’m a CL \\nI’m available',\n",
       " \"I'm in NC on market support...I'm not desperate...but dam I worked for it...when I'm late I get docked or fired...oh but tech Fri is pay day so we can't say sh!t\",\n",
       " '@luke_lazar @Shawn_Fahey @Scott_Richwine @Alexander_Amerman @Justin_Close @Marlin_Rainey @Adam_Wintersteen @David_Wildey @tamara_palmer1',\n",
       " \"@client_services_transition that will work they aren't doing a big zone they are going to be working in 1 hole\",\n",
       " 'Me',\n",
       " 'Yes',\n",
       " 'Very cute',\n",
       " 'I’m Available with mentee bro',\n",
       " 'I can go',\n",
       " 'Available',\n",
       " \"I heard that, but Joe said that we have to take it to Firestone because they're supposed to rotate our tires, but I've never seen that happen.\",\n",
       " 'Definitely let the cops handle it..',\n",
       " 'Available',\n",
       " \"Welcome y'all\",\n",
       " 'Prayers for you and your family',\n",
       " \"I'm available\",\n",
       " 'I send my condolences to the family of both workers. We will pray for healing for the cm that was also hit. I have had a couple close calls and it is very scary. You always have to be looking at traffic constantly.',\n",
       " 'Available',\n",
       " 'Me',\n",
       " '@Nicholas_Watkins',\n",
       " \"Be in the moment in your travels! Being cognitively distracted is a real thing and it's dangerous to let our minds wander.\",\n",
       " 'Call they got plenty of orders',\n",
       " '@Alexander_Amerman...thoughts? Is this what you were looking for?',\n",
       " 'Available',\n",
       " \"We recommend calling and selecting option '6' for technology support rather than the HR queue! Hopefully, you've been able to find a resolution, @Connard_Edlin.\",\n",
       " '@Dylan_Claycomb np man happy to help',\n",
       " 'Available',\n",
       " 'Available',\n",
       " '@Clay_Morris is awesome! I get a tear in my when I think about his positive success. I will tell you that mistakes only happen once with him. \\n\\n     One thing that stands out is when he takes his lumps, it is not about him; it is about his family. He does not want to disappoint those who love and cherish him at home. He is a good man and father! Keep shining sir!',\n",
       " '@Toby_Germinario @Lance_Fountain',\n",
       " 'Yes I did and yes I got them back today',\n",
       " 'Me again',\n",
       " 'Same',\n",
       " 'What is the name of the company that we use to sign in? Traffic Control Services LLC, TCSLLC or Flagger Force?',\n",
       " 'I’m done just gotta get my ap',\n",
       " '@Quinten_Ford doesn’t want to go, can I go instead? My ETA is 40 minutes',\n",
       " 'Oh man that must have been something I put in during training a while ago, I should be out with a field trainer tomorrow and Tuesday actually I just finished lead. Please remove UPTO, I’ll get in touch with my area supervisor I didn’t realize that is probably why I’m not dispatched with a FT tomorrow. If I cannot get with one tomorrow I will take quakertown. Will get back to you asap. TY',\n",
       " 'What are the hours',\n",
       " 'Is this filled',\n",
       " 'Shelby',\n",
       " \"@Charles_Yates and @John_Ware I will add you to the order and you can be on your way. Please call the office and let us know what PLT's you have. Thank you so much and I apologize for the delay.\",\n",
       " 'me too',\n",
       " \"I got called to come to Nescopeck. I live 15 mins but I told them I'd be here in an hour I needed to dig out my truck. All the crews went out. My crew member was  told to have me call the foreman when I got here. I called, left a message, called a second time and he said he'll call me back with a plan of what he wants me to do.. that was at 10:24 this morning. I still sit here.. yeah making money but.. I hate not knowing expectations.\",\n",
       " '@Jeremy_Greene yes I agree. If I get the pleasure of working with you I’d love to learn some asl if we get the moment to do so.',\n",
       " 'State College does',\n",
       " 'Not a problem',\n",
       " '@Latece_Greer my fault... you are correct. TDOC showed me you had 2 jobs today but I refreshed. Any specific job for tomorrow your interested in? The available orders listed above are all pretty far away from you',\n",
       " 'Avail',\n",
       " \"It's also unsafe to have your cruise control on when driving in icy or wet weather conditions\",\n",
       " 'What time would I get done work tonight',\n",
       " 'Ok',\n",
       " 'Yea I’m in Spartanburg now',\n",
       " '@Grace_Billing they will give you a job don’t stress u can also let the as of that area kno and he/she may have sum for u',\n",
       " 'Available',\n",
       " 'We will review Inactive Refusal points that may be related to this issue on Monday.  There is no need to call Employee Services over the weekend about this.  Again, we thank you for your patience.',\n",
       " 'Can I get on wire watch tomorrow nite 2300 @weekend_dispatch at peco',\n",
       " 'Sorry if this is something that has caused you frustration, Lance. The best thing would be to call into our payroll support team at the OSC and explain your situation to them.',\n",
       " \"Dam Patrick \\nSounds like me I've only got 18 hours for the week\",\n",
       " 'I emailed 📧',\n",
       " \"CONGRATULATIONS 🎉🎉🎉🎉 \\nWoW! Unexpected,  Positive Acknowledgement is GREAT!!!! To think...🤔, I've worked with some of you! Whew!!!\\n\\nCongratulations, Once Again!!! 👏👏👏👏👏👏👏👏👏👏👏👏👏👏\",\n",
       " 'Hop from one spot that 20 minutes the next one 45 lmao 😂 where my money run me y’all pockets 😂',\n",
       " 'Condolences and prayers',\n",
       " '@Brian_Litzinger might be interested in the somerset order',\n",
       " 'U got urself a dodge',\n",
       " 'You ok',\n",
       " 'Morning Connard Edlin',\n",
       " 'It may be the mail system too, @Kelly_Sweigart . I live outside of Altoona, and as of yesterday I stil havent received mine.\\n\\nI replied on another thread about the same concern.\\nSomeone from payroll will probably tell you they resent one to be safe.',\n",
       " 'There was a bridge in California that the harmonics of the wind made it buckle  and collapse.  I saw it on video when I was in college... 20 something years ago.. technology fixed some of the construction',\n",
       " 'Welcome to Flagger Force, Stay safe',\n",
       " 'Thank you all I truly appreciate the L\\U0001faf6🏾VE  \\nAnd always remember ( I’m available for anything always ready to slide on any block 😂\\U0001fae1👷🏾\\u200d♂️) lol Thanks everyone 🦾',\n",
       " 'I can go as cm',\n",
       " 'yes sir!',\n",
       " 'If you still have a cm spot I’m available',\n",
       " 'This could be staged just for the photo',\n",
       " '520535',\n",
       " 'You too',\n",
       " 'You had a bad day The Sun Will come out Tomorrow',\n",
       " 'Thank you all so much.  I truly enjoy working with this company and feeling like a valued member of a team.',\n",
       " 'My prayers are with you and your family I hope she gets better soon',\n",
       " 'U need for day time 2 trucks and a few cm at least 3 if 2 trucks',\n",
       " 'Available',\n",
       " '@Caleb_Johnson',\n",
       " 'Okay thanks. Yes I used the same information from the dispatch app using the ff email to sign in and it still says I’m not authorized',\n",
       " 'Available',\n",
       " \"🤣😂 OmG, youre getting paid. You're at the garunteed 4 hours.\\n\\nWhat are you going to do if youre ever on storm duty? Complain about sitting there too???\",\n",
       " 'Make sure to coordinate with your AS, @Cheyanne_Carlson',\n",
       " 'Had a blast with this group! Way to rock!',\n",
       " 'Me Edward holmes',\n",
       " '@david_keenan They got rid of him, what happened?',\n",
       " 'Nope just sleet, rain,  and ice',\n",
       " '🤦🏻\\u200d♂️SMH 🤣',\n",
       " 'Dameon Watkins',\n",
       " 'I’m available for work tomorrow as well',\n",
       " 'I’ll go as a CM',\n",
       " 'Get some',\n",
       " 'Available',\n",
       " '@Glenn_Streets welcome to the family your in good hands and always prepare yourself for everything winter summer fall spring rain gear etc',\n",
       " \"Congratulations to you, @amber_fatta!  It's always a joy working with you.  You rock, my friend!\",\n",
       " '@Ashley_Barnhouse are you in the area of South Carolina?',\n",
       " 'Available',\n",
       " 'I may hold the paddle close in between watching for traffic and if it is windy',\n",
       " \"@Kelly_Aitchison there really not ones to talk to, if I was you I would be contact other AS's FS' and our fellow other crew leads and advance crew leads, now if the gas station provides a car wash after getting gas then yes ho for it but that is not the only way to have our trucks washed.\",\n",
       " 'Same available ',\n",
       " \"I don't have a job tomorrow I can be there by the time the LPTs\",\n",
       " 'Ready',\n",
       " 'Each market has their own trainer. Everything you listed may have not even been covered in another market.\\n\\nThey never showed my class how to check pay stubs, how to request off, (we dont get breaks in PA unless theres more than 3)\\n\\nWhenever I get a new CM, I ask them flat out what they want to get out of the company, and how things were explained with previous leads',\n",
       " '@Kanestra_Durant i agree with you. If i never called osc for another job i would of never worked yesterday… and today my job got canceled. I just called to see if they had any jobs but of course they didn’t.',\n",
       " 'Available',\n",
       " '👍welcome aboard',\n",
       " 'Congratulations',\n",
       " 'Congratulations I look forward to working with you one day',\n",
       " 'Thanks, Team Awesome!',\n",
       " 'Available',\n",
       " 'Good point,🤣',\n",
       " 'Me',\n",
       " 'Still available?',\n",
       " 'I’ll go',\n",
       " '@Stephen_AndrezeywskiJr   Inptcs is the short code ',\n",
       " \"I'll be available after 4pm today\",\n",
       " 'A great shoutout, @Matthew_Gregory! Keep up the hard work, @Felicia_Hamilton ask questions, and use what you know going into BWZ.',\n",
       " 'Me',\n",
       " 'Me',\n",
       " 'No offense taken here, was just looking to be a part of a discussion that I think is important to the safety of others',\n",
       " 'The follow vehicle has been found, looking for 1 ACL (SEO cert) to drive the stake body from Glen Burnie',\n",
       " '@Jennifer_Grant   Please  have your friend reach out to recruitment by calling the OSC opt#  8.    There are requirements that have to be met to qualify for mentor/mentee program.   Also, depending where she lives we might not be hiring in that area.',\n",
       " '@Kathy_Wise heck yea we the dream team lol',\n",
       " '@FlaggerForce may the forth be with you',\n",
       " 'I got 3 from yesterday 1 for refusal and 2 for active refusal yet I had my ingrown toenail removed they wanted me off working today and yesterday but I didn’t want any points anymore',\n",
       " 'I can go if u need a cl',\n",
       " 'Please im available',\n",
       " 'We would like to honorarily announce @David_Keenan as the first person to have said this!',\n",
       " \"@Tyler_Young let's get it all you gotta do is go across the street lol\",\n",
       " \"@Kathleen_Cleveland did the client talk to your AS or call the OSC? That could be the problem. If the client never tells the OSC they can't cancel the order.\",\n",
       " 'Big Rob 💪💪💪...',\n",
       " 'Congrats!!!',\n",
       " 'Dominique Petel... Available anytime.',\n",
       " '@Charles_Yates you can’t do the order because of being on a TMA job in the morning',\n",
       " 'Available',\n",
       " 'I used to go to wawa. It was free.',\n",
       " 'Very much appreciated',\n",
       " 'Welcome to the family',\n",
       " 'I’m available',\n",
       " '@John_Barrett is @Gary_Williams aware he would be added to this order?',\n",
       " 'With mentee',\n",
       " 'Congratulations',\n",
       " 'Good morning  from Pennsylvania',\n",
       " 'No crewmember',\n",
       " 'Put me and my mentee on take me off my Cumberland order please',\n",
       " 'For today',\n",
       " 'This is a great example of the critical problem-solving skills that befit the great leaders in this company. Pairing that with excellent communication leads to a day of safety and success for our clients!',\n",
       " 'Me',\n",
       " \"@Tiffany_Silver sorry you're buying so many batteries. I bought rechargeable battery packs so I can recharge my radio while I use my spare.\",\n",
       " \"I always let the cm ride with me from meeting spot to job site. Just make sure their car is in safe parking spot. I'm an ACL so don't let anyone tell you your not aloud to ride in truck with them.\",\n",
       " '@Donnie_Ziegler She wouldnt tell us if she was extra crispy or original recipe 🤷🏻\\u200d♂️',\n",
       " 'Available, I also messaged you @James_Malec',\n",
       " 'Thoughts and prayers for ya. We will see you soon. Get healthy.',\n",
       " 'Me too🤟🏾🤟🏾🤟🏾',\n",
       " 'Wednesday morning. I said wtf',\n",
       " 'Did this fill',\n",
       " \"Jumped the gun... sorry guys can't make this one\",\n",
       " '@Ronald_Eichelberger, thank you so much!  I do not have a \"list\", but if you could please visit the Flagger Force Impacts portal to sign up and reach out directly to the contact at Make-A-Wish who is organizing the event.   I will send you a private message with instructions.  And again, THANK YOU!',\n",
       " 'I will go if u need call me',\n",
       " 'Wish I could help 😪',\n",
       " \"@Sarah_Rice that is such a well said post. I love it. Team work makes the dream work. I'm just thankful to have such an amazing company to work for, and it's values are very important to me. Thanks for the words of wisdom today. Hope all my ff family has a wonderful day....\",\n",
       " 'I’m about 2 hrs away but I’m available',\n",
       " 'Nick st orge',\n",
       " \"I didn't have my foot on the gas was going below 50mph trying to make more travel distance when it spoke to me. Went down to 45mph and it said it again. I was on I80 with cars and trucks speeding past. Braking or changing  lanes wasn't possible.\",\n",
       " 'Congrats on cl!',\n",
       " 'Sorry I missed it everyone hope you all had a good time',\n",
       " 'I am available.',\n",
       " 'How long will it be',\n",
       " '@Toby_Germinario SAD 😔',\n",
       " 'Indeed I like working with her',\n",
       " 'I almost had a panic attack 🤦🏾\\u200d♀️ I can’t my streets are not cleaned off yet the further down I went from my house just got worse I felt the back of my truck start to slide then I realized naw this is my sign to find a safe way to get back I guess next time I have a job I’ll just risk my life and go and see what happens from the outcome.',\n",
       " 'I do both I pack my and sometimes I buy my lunch',\n",
       " '🙏',\n",
       " \"There are no gas stations in the Williamsport Area that have a gas station associated with it. I've been driving to Sheetz after work in Muncy one day a week which is a 20-30 minute drive from my home as they do have one on premises to comply with company policy. I haven't tried to run the card at other car washes in town as we were told to get it washed and Pay for it when we got gas.\",\n",
       " 'Congratulations Elliott',\n",
       " \"That was my fault osc was suppose to keep the guys on the order on for the weekend. If I need a cm in the future I'll have my AS give you a call.\",\n",
       " 'Hi @Joshua_Miller, from a browser page on your mobile device, please go to the following webpage for further instruction/assistance:\\n\\n[www.flaggerforce.com/apps]\\n\\nScroll down to find the app, instructions, and tutorial for the OS of your respective mobile device.',\n",
       " 'I can work Saturday and Sunday morning',\n",
       " 'I’m in Md but it don’t take too long from where I live to get to the pa line',\n",
       " 'Team work',\n",
       " \"@Manuel_Megofna, I don't think he was here working. I saw a post of his Friday that said he was coming to NC and he is back to PA already.\",\n",
       " 'Available',\n",
       " 'You are one of the best! Have an amaazing New Year! Looking forwar to the energyyou bring every day!',\n",
       " 'I just had and oil change like a month a ago in Monroeville',\n",
       " \"@Jessica_Lee I've  called twice an left a voicemail about a loaner no response! They don't care\",\n",
       " 'Congratulations to the whole crew, so proud of you @Jade_Rowe 👏 and @Anthony_Mills they had an amazing instructors helping them along the way!!!',\n",
       " 'I know',\n",
       " 'Thank you @Tianna_Mongold @Johnny_Vieira ',\n",
       " \"There's a whole story revolving around these two individuals on Tik Tok! Both the painter and the worker ended up going back in forth over time, never realizing each other's presence until the story became so popularized that they finally met!\",\n",
       " 'Me',\n",
       " 'Can I?',\n",
       " 'I love Casey clips',\n",
       " 'Thanks😎',\n",
       " \"Big Gas leaks out here...it's already 3 work sights...\",\n",
       " 'Condolences to her family and friends',\n",
       " \"This is why we need to make sure we're watching everything and making sure everyone is properly trained. We have lost 2 people in the past 6 months and I honestly feel like it has to do with us rushing things.\",\n",
       " 'I’m available and close 10mins away!!!!!',\n",
       " 'Me',\n",
       " 'Road closure already set up come get this easy money..',\n",
       " 'We are not like every other company. If you research bonuses and how they are paid out; you would find that some states have a mathmatical formula that is used to figure out the amount of payout. \\n\\nThat formula might make the payment less than a taxed bonus.\\n\\n     The payout is indicated on our paystubs very clearly. If you have a question about it, reach out to payroll and they can assist you with this question. Those folks have woked diligently to make sure that our pay is correct. Yes, mistakes happen. Make the call. Do research!',\n",
       " '***FILLED**',\n",
       " \"That's what I am getting when reading they can tell you yes you go do the job and then corporate can say no we will keep that 🤔\",\n",
       " \"That's cool but my comment was for a just incase basis. If they end up needing a crew member I'm available. I know what the job is saying they need. I'm just putting myself out there. Thanks.\",\n",
       " 'Im interested',\n",
       " 'Right! Lol',\n",
       " 'Respect',\n",
       " 'It will be 8 hours, I talked to the client about the hours',\n",
       " 'They have offered it and I said yes a few months back still waiting',\n",
       " 'Thank you @tamara_palmer1 !',\n",
       " 'Thanks',\n",
       " 'It was a pleasure working beside Moe as a crew member can’t wait to work beside her as a crew lead',\n",
       " \"I've always loved this analogy.  Thanks for this. 😌 @Anthony_Mills\",\n",
       " 'It should state that in the post. It\\'s very misleading. \"In your area\"?',\n",
       " 'Word',\n",
       " 'Hello @Rodney_Sydnor1  you have been added to order #578875.  Thank you for your help.',\n",
       " 'Available but is 2 hours drive from',\n",
       " '@Shonda_Smith yes they were watching each other’s backs and taking turns sweeping the glass out the roadway.',\n",
       " 'We had a bunch of people cutting in front of us and we were  doing about 10mph over the limit.',\n",
       " 'Me plz',\n",
       " 'Congratulations and welcome to the CL life and time Can’t wait to see you shining your position. Hope to see you on a site one day. Stay safe and enjoy',\n",
       " \"@client_services_transition I'm available!! And I understand the per diem policy as well. I need hours\",\n",
       " \"Hi @Phillip_LaBeau, if you haven't done so already, please make sure you are sending an email to EmployeeServices@FlaggerForce.com\",\n",
       " 'Available',\n",
       " '💲🤙👷\\u200d♀️💪🤣',\n",
       " 'Me',\n",
       " 'How long will they be out there?',\n",
       " 'Me',\n",
       " 'Me',\n",
       " 'Congratulations @Shannon_Crothers 👏👏🎉',\n",
       " '😂😂😂',\n",
       " 'Thanks Charles, I appreciate it 💯☮️🎆💪🏽🙏🏾',\n",
       " 'I have a job already tomorrow so do I get the double pay for it ? @weekend_dispatch',\n",
       " 'The start time will be on the order',\n",
       " 'I’m available',\n",
       " 'If I get 1 day you can get unemployment',\n",
       " 'ok, thanks...if you get closer, lmk',\n",
       " 'I’m available',\n",
       " 'Why this makes me giggle every time I see it I’ll never know haha 😂😂😂',\n",
       " \"mornin' toby, stay safe\",\n",
       " '@client_services_transition lmk',\n",
       " 'Looks great! Can’t wait to hear how it goes!',\n",
       " \"I'm sorry for your loss\",\n",
       " 'If you  still need someone  l can do it. 10 minutes to the Central office Middletown area',\n",
       " 'Remindersss listen up',\n",
       " 'Stay positive and learn all you can brother',\n",
       " 'Available',\n",
       " \"Any chance we'll get a soup preview @paula_rosen?\",\n",
       " 'I am currently here in Winston Salem for the travel work at a hotel I can pick up a Saturday shift and go back home after put me in',\n",
       " 'Welcome to the family! Have a great first day!',\n",
       " 'Any room for CM',\n",
       " '@Delas_Brunson @Cody_Carpenter @Brady_Macklin',\n",
       " '@Rodney_Monroe',\n",
       " 'Congrats to all!  Good luck!',\n",
       " 'Montoursville',\n",
       " 'Me',\n",
       " \"Where's the jobs located at\",\n",
       " 'Teamwork made the dreamwork bro pleasure was all mine',\n",
       " 'Question if you can not put anything on the sign legs ? Why can we use cones']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to prepare the data to be loaded to the database\n",
    "def prepare_data(df):   \n",
    "    documents = []\n",
    "    metadatas = []\n",
    "    ids = []\n",
    "        \n",
    "    for idx, row in df.iterrows(): \n",
    "        documents.append(row.iloc[4])\n",
    "        metadatas.append({\n",
    "            'stream_name': row.iloc[0],\n",
    "            'created_date': row.iloc[1],\n",
    "            'post_id': row.iloc[2],\n",
    "            'comment_id': row.iloc[3],\n",
    "            'like_count': row.iloc[5],\n",
    "            'report_count': row.iloc[6],\n",
    "            'username': row.iloc[7],\n",
    "            'author_user_id': row.iloc[8],\n",
    "            'author_position': row.iloc[9],\n",
    "            'author_status': row.iloc[10]\n",
    "        })\n",
    "        ids.append(str(idx+1))       \n",
    "    \n",
    "    return documents, metadatas, ids\n",
    "\n",
    "documents, metadatas, ids = prepare_data(df_comments_to_test)\n",
    "\n",
    "print(type(documents))\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stream_name': 'Safety & Operations',\n",
       " 'created_date': '2022-12-28 13:18:45',\n",
       " 'post_id': 6113990,\n",
       " 'comment_id': 5205991,\n",
       " 'like_count': 3,\n",
       " 'report_count': 0,\n",
       " 'username': 'FlaggerForce',\n",
       " 'author_user_id': 'b701ab9f-563a-4425-a389-aff803a8da58',\n",
       " 'author_position': nan,\n",
       " 'author_status': 'active'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadatas[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data uploaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "#from langchain_chroma import Chroma\n",
    "#from langchain_community.embeddings import OllamaEmbeddings\n",
    "#import chromadb.utils.embedding_functions as embedding_functions\n",
    "from chromadb.config import Settings\n",
    "from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "      \n",
    "PERSIST_DIRECTORY = \"C:\\\\Users\\\\eduar\\\\Documents\\\\Master_Thesis\\\\GenAI_Thesis_Beekeeper\\\\data\\\\datasets_db\"\n",
    "embeddings = SentenceTransformerEmbeddings(model_name=\"all-mpnet-base-v2\")    \n",
    "    \n",
    "\n",
    "# Function to create and load data to the database\n",
    "\"\"\"\n",
    "The embedding function takes text as input, and performs tokenization and embedding. If no embedding function is supplied, Chroma will use sentence transformer as a default.\n",
    "https://docs.trychroma.com/embeddings#sentence-transformers\n",
    "By default, Chroma uses all-MiniLM-L6-v2\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def upload_to_chromadb(documents, metadatas, ids):\n",
    "    try:\n",
    "        # Initialize ChromaDB client\n",
    "        #chromadb_client = chromadb.EphemeralClient()      \n",
    "        chromadb_client = chromadb.PersistentClient(path=PERSIST_DIRECTORY, settings=Settings(allow_reset=True))\n",
    "        \n",
    "        # create the open-source embedding function\n",
    "        # embedding = OllamaEmbeddings(model=\"mistral\")\n",
    "        embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")   # Default\n",
    "        embeddings = SentenceTransformerEmbeddings(model_name=\"all-mpnet-base-v2\")          # Best model from included in ChromaDB\n",
    "               \n",
    "        # Create a comment's collection\n",
    "        collection_comments = chromadb_client.create_collection(name=\"comments_collection\",\n",
    "                                                              metadata={\"hnsw:space\": \"l2\"}, # Squared L2 norm(l2) is the default, inner product('ip') or cosine similarity('cosine')                                                              \n",
    "                                                              #embedding_function=embeddings\n",
    "                                                              )        \n",
    "        \n",
    "        # Add data to collection\n",
    "        collection_comments.add(documents=documents,\n",
    "                                metadatas=metadatas,\n",
    "                                ids=ids\n",
    "                                )\n",
    "        print(\"Data uploaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to upload data: {e}\")\n",
    "\n",
    "upload_to_chromadb(documents, metadatas, ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'slice'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m result \u001b[38;5;241m=\u001b[39m comments_collection\u001b[38;5;241m.\u001b[39mget()\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(result))\n\u001b[1;32m---> 12\u001b[0m \u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'slice'"
     ]
    }
   ],
   "source": [
    "# Get the first 10 documents in comments_collection\n",
    "# Initialize database\n",
    "PERSIST_DIRECTORY = \"C:\\\\Users\\\\eduar\\\\Documents\\\\Master_Thesis\\\\GenAI_Thesis_Beekeeper\\\\data\\\\datasets_db\"\n",
    "chromadb_client = chromadb.PersistentClient(path=PERSIST_DIRECTORY, settings=Settings(allow_reset=True))\n",
    "comments_collection = chromadb_client.get_collection(name=\"comments_collection\")\n",
    "chromadb_client.heartbeat()\n",
    "\n",
    "\n",
    "# Only get documents and ids\n",
    "result = comments_collection.get()\n",
    "print(len(result))\n",
    "result[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ids', 'embeddings', 'metadatas', 'documents', 'uris', 'data'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Deleting the client\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "PERSIST_DIRECTORY = \"C:\\\\Users\\\\eduar\\\\Documents\\\\Master_Thesis\\\\GenAI_Thesis_Beekeeper\\\\data\\\\datasets_db\"\n",
    "\n",
    "chromadb_client = chromadb.PersistentClient(path=PERSIST_DIRECTORY, settings=Settings(allow_reset=True))\n",
    "chromadb_client.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the collection\n",
    "chromadb_client.delete_collection(name=\"comments_collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['82239', '47464', '129219']],\n",
       " 'distances': [[1.0975472927093506, 1.3787870407104492, 1.386555552482605]],\n",
       " 'metadatas': None,\n",
       " 'embeddings': None,\n",
       " 'documents': [['Very well written post!!!!!  I absolutely love this. ❤ I am truly thankful for the wonderful teamwork that is displayed every day and for being a part of the FF Family',\n",
       "   'What a beautiful set up!!! GREAT JOB everyone!!!!!',\n",
       "   'Congratulations @Roniece_McFadden keep going on up that ladder. You are going to do great out there.']],\n",
       " 'uris': None,\n",
       " 'data': None}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "# Use the query method to search for similar comments\n",
    "text = \"We appreciate your support\"\n",
    "#username = 'Ebony_Scott'\n",
    "#username = 'John_Ware'    # User with many comments\n",
    "username = 'FlaggerForce'\n",
    "num_comments = 3\n",
    "min_num_likes = 3\n",
    "\n",
    "# Define a function to query the database\n",
    "def query_database(text, username, num_comments=3, min_num_likes=3):\n",
    "    try:\n",
    "        # Initialize database\n",
    "        chromadb_client = chromadb.PersistentClient(path=PERSIST_DIRECTORY, settings=Settings(allow_reset=True))\n",
    "        comments_collection = chromadb_client.get_collection(name=\"comments_collection\")\n",
    "        \n",
    "        # Query the database\n",
    "        context_docs = comments_collection.query(\n",
    "            query_texts=[text],\n",
    "            n_results=num_comments,\n",
    "            include=['distances','documents'],\n",
    "            #where={\"$and\": [{\"username\": username}, {\"like_count\": {\"$gt\": min_num_likes}}]}\n",
    "            where={\"like_count\": {\"$gt\": min_num_likes}}\n",
    "        )\n",
    "        return context_docs\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to query the database: {e}\")\n",
    "        \n",
    "# Query the database\n",
    "retrieved_docs = query_database(text, username, num_comments, min_num_likes)\n",
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Very well written post!!!!!  I absolutely love this. ❤ I am truly thankful for the wonderful teamwork that is displayed every day and for being a part of the FF Family',\n",
       "  'What a beautiful set up!!! GREAT JOB everyone!!!!!',\n",
       "  'Congratulations @Roniece_McFadden keep going on up that ladder. You are going to do great out there.']]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs['documents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Very well written post!!!!!  I absolutely love this\\n\\n❤ I am truly thankful for the wonderful teamwork that is displayed every day and for being a part of the FF Family\\n\\nWhat a beautiful set up!!! GREAT JOB everyone!!!!!\\n\\nCongratulations @Roniece_McFadden keep going on up that ladder. You are going to do great out there'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split documents into chunks\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "import chromadb\n",
    "\n",
    "# Define a function to split and format the documents\n",
    "def split_documents(documents):\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "        separator=\".\",\n",
    "        chunk_size=100, \n",
    "        chunk_overlap=0)\n",
    "    splits = text_splitter.create_documents(documents['documents'][0])\n",
    "    return splits\n",
    "\n",
    "splitted_docs = split_documents(retrieved_docs)\n",
    "splitted_docs\n",
    "\n",
    "\n",
    "# Function to format documents\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "formatted_context = format_docs(splitted_docs)\n",
    "formatted_context\n",
    "# # create the open-source embedding function\n",
    "# embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# langchain_chroma = Chroma(\n",
    "#     client=chromadb_client,\n",
    "#     collection_name=\"comments_collection\",\n",
    "#     embedding_function=embedding_function,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 159, which is longer than the specified 100\n",
      "Created a chunk of size 150, which is longer than the specified 100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Thank you for your kind words and appreciation, Corey! I'm glad that our teamwork and collaboration have made a positive impact on your experience at FF. It's always a pleasure to work together and support each other in our daily tasks. Your story about the light towers and bathroom break is a great example of how we prioritize safety and teamwork, even during challenging situations.\\n\\nI'm also glad that you found our Pittsburgh-style welcoming and decided to give social media a try! We're excited to have you as part of our team, and always prioritize safety above all else. Let's continue working together to make FF the best it can be! Go Steelers! 😉🏈💪🏼 #FFFamily #TeamworkMakesTheDreamWork\""
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "import ollama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "# Function that defines the RAG chain\n",
    "def rag_chain(question):\n",
    "    retrieved_docs = query_database(question, username, num_comments, min_num_likes)\n",
    "    splitted_docs = split_documents(retrieved_docs)\n",
    "    formatted_prompt = format_docs(splitted_docs)\n",
    "    response = ollama.chat(model='mistral', messages=[{'role': 'user', 'content': formatted_prompt}])\n",
    "    return response['message']['content']\n",
    "\n",
    "# Test the RAG chain\n",
    "question = \"Make a welcome post for Daniel Meier who has just started a new position as Crew Leader\"\n",
    "response = rag_chain(question)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['82239', '47464', '129219']],\n",
       " 'distances': [[1.0975472927093506, 1.3787870407104492, 1.386555552482605]],\n",
       " 'metadatas': None,\n",
       " 'embeddings': None,\n",
       " 'documents': [['Very well written post!!!!!  I absolutely love this. ❤ I am truly thankful for the wonderful teamwork that is displayed every day and for being a part of the FF Family',\n",
       "   'What a beautiful set up!!! GREAT JOB everyone!!!!!',\n",
       "   'Congratulations @Roniece_McFadden keep going on up that ladder. You are going to do great out there.']],\n",
       " 'uris': None,\n",
       " 'data': None}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize database\n",
    "chromadb_client = chromadb.PersistentClient(path=PERSIST_DIRECTORY, settings=Settings(allow_reset=True))\n",
    "comments_collection = chromadb_client.get_collection(name=\"comments_collection\")\n",
    "\n",
    "# Use the query method to search for similar comments\n",
    "text = \"We appreciate your support\"\n",
    "#username = 'Ebony_Scott'\n",
    "#username = 'John_Ware'    # User with many comments\n",
    "username = 'FlaggerForce'\n",
    "num_comments = 3\n",
    "min_num_likes = 3\n",
    "\n",
    "# Query the database\n",
    "context_docs = comments_collection.query(    \n",
    "    query_texts=[text],\n",
    "    n_results=num_comments,\n",
    "    include=['distances','documents'],\n",
    "    #where={\"$and\": [{\"username\": username}, {\"like_count\": {\"$gt\": min_num_likes}}]}\n",
    "    where={\"like_count\": {\"$gt\": min_num_likes}}\n",
    ")\n",
    "\n",
    "context_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# text = \"\"\"\n",
    "# I think it is about time to find a more interesting job off the road. I always wanted to switch to a position with a higher administrative load of work, what do you think about it?\n",
    "\n",
    "# Text models specially those centered around conversational chatbots have revolutionized AI since the launch of ChatGPT. Helped by natural language processing and large language models, these models have many very useful capabilities like summarization, writing assistance, code generation, language translation and sentiment analysis. \n",
    "\n",
    "# They have been the main focus in Generative AI because of the capabilities of ChatGPT, a application which millions of users are already taking advantage of.\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# Initialize database\n",
    "chromadb_client = chromadb.PersistentClient(path=PERSIST_DIRECTORY, settings=Settings(allow_reset=True))\n",
    "comments_collection = chromadb_client.get_collection(name=\"comments_collection\")\n",
    "\n",
    "# Use the query method to search for similar comments\n",
    "text = \"job\"\n",
    "#username = 'Ebony_Scott'\n",
    "username = 'Aaron_Burch'    # Most popular user, Advanced Crew Leader\n",
    "num_comments = 5\n",
    "min_num_likes = 1\n",
    "context_docs = comments_collection.query(query_texts=[text],\n",
    "                                      n_results=num_comments,\n",
    "                                      include=['distances','documents'],\n",
    "                                      where={\"username\": username, \n",
    "                                             \"like_count\": {\"$gt\": min_num_likes}\n",
    "                                             },\n",
    "                                      #where_document={\"$contains\":\"congratulations\"},\n",
    "                                      )\n",
    "\n",
    "context_docs\n",
    "\n",
    "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=0)\n",
    "# splits = text_splitter.create_documents([text])\n",
    "# splits\n",
    "\n",
    "# comments_collection.query(\n",
    "#     query_texts=[\"doc10\", \"thus spake zarathustra\", ...],\n",
    "#     n_results=10,\n",
    "#     where={\"metadata_field\": \"is_equal_to_this\"},\n",
    "#     where_document={\"$contains\":\"search_string\"}\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can load the persisted database from disk, and use it as normal. \n",
    "from langchain.vectorstores import Chroma\n",
    "vectordb = Chroma(persist_directory=PERSIST_DIRECTORY)\n",
    "retriever = vectordb.as_retriever()\n",
    "docs = retriever.get_relevant_documents(\"find a new job\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize database\n",
    "chroma_client = chromadb.PersistentClient(path=\"C:\\\\Users\\\\eduar\\\\Documents\\\\Master_Thesis\\\\GenAI_Thesis_Beekeeper\\\\data\\\\vectordb\")\n",
    "collection_comments = chroma_client.get_collection(name=\"comments_collection\")\n",
    "# print(collection_comments)\n",
    "\n",
    "results = collection_comments.query(    \n",
    "    query_texts=[\"find a job\"],\n",
    "    n_results=3,\n",
    "    include=['distances','documents']\n",
    ")\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "# Initialize database\n",
    "chroma_client = chromadb.PersistentClient(path=\"C:\\\\Users\\\\eduar\\\\Documents\\\\Master_Thesis\\\\GenAI_Thesis_Beekeeper\\\\data\\\\vectordb\")\n",
    "chroma_client.delete_collection(name=\"comments_collection\")\n",
    "# chroma_client.reset() # Empties and completely resets the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "text = \"\"\"\n",
    "I think it is about time to find a more interesting job off the road. I always wanted to switch to a position with a higher administrative load of work, what do you think about it?\n",
    "\n",
    "Text models specially those centered around conversational chatbots have revolutionized AI since the launch of ChatGPT. Helped by natural language processing and large language models, these models have many very useful capabilities like summarization, writing assistance, code generation, language translation and sentiment analysis. \n",
    "\n",
    "They have been the main focus in Generative AI because of the capabilities of ChatGPT, a application which millions of users are already taking advantage of.\n",
    "\"\"\"\n",
    "documents = collection_comments.query(query_texts=[text],\n",
    "                                      n_results=3,\n",
    "                                      include=['distances','documents'])\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=0)\n",
    "splits = text_splitter.create_documents([text])\n",
    "splits\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"mistral\")\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and retrieve documents from ChromaDB\n",
    "def load_and_retrieve_docs(query):\n",
    "    # Retrieve documents based on a simple text query or other metadata-based retrieval\n",
    "    documents = collection_comments.query(query)\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    splits = text_splitter.split_documents([doc['content'] for doc in documents])\n",
    "    embeddings = OllamaEmbeddings(model=\"mistral\")\n",
    "    vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)\n",
    "    return vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with lang chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1714979865447440200"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "PERSIST_DIRECTORY = \"C:\\\\Users\\\\eduar\\\\Documents\\\\Master_Thesis\\\\GenAI_Thesis_Beekeeper\\\\data\\\\datasets_db\"\n",
    "\n",
    "# Initialize database\n",
    "PERSIST_DIRECTORY = \"C:\\\\Users\\\\eduar\\\\Documents\\\\Master_Thesis\\\\GenAI_Thesis_Beekeeper\\\\data\\\\datasets_db\"\n",
    "chromadb_client = chromadb.PersistentClient(path=PERSIST_DIRECTORY, settings=Settings(allow_reset=True))\n",
    "comments_collection = chromadb_client.get_collection(name=\"comments_collection\")\n",
    "chromadb_client.heartbeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'license': '                                 Apache License\\n                           Version 2.0, January 2004\\n                        http://www.apache.org/licenses/\\n\\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\\n\\n   1. Definitions.\\n\\n      \"License\" shall mean the terms and conditions for use, reproduction,\\n      and distribution as defined by Sections 1 through 9 of this document.\\n\\n      \"Licensor\" shall mean the copyright owner or entity authorized by\\n      the copyright owner that is granting the License.\\n\\n      \"Legal Entity\" shall mean the union of the acting entity and all\\n      other entities that control, are controlled by, or are under common\\n      control with that entity. For the purposes of this definition,\\n      \"control\" means (i) the power, direct or indirect, to cause the\\n      direction or management of such entity, whether by contract or\\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\\n      outstanding shares, or (iii) beneficial ownership of such entity.\\n\\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\\n      exercising permissions granted by this License.\\n\\n      \"Source\" form shall mean the preferred form for making modifications,\\n      including but not limited to software source code, documentation\\n      source, and configuration files.\\n\\n      \"Object\" form shall mean any form resulting from mechanical\\n      transformation or translation of a Source form, including but\\n      not limited to compiled object code, generated documentation,\\n      and conversions to other media types.\\n\\n      \"Work\" shall mean the work of authorship, whether in Source or\\n      Object form, made available under the License, as indicated by a\\n      copyright notice that is included in or attached to the work\\n      (an example is provided in the Appendix below).\\n\\n      \"Derivative Works\" shall mean any work, whether in Source or Object\\n      form, that is based on (or derived from) the Work and for which the\\n      editorial revisions, annotations, elaborations, or other modifications\\n      represent, as a whole, an original work of authorship. For the purposes\\n      of this License, Derivative Works shall not include works that remain\\n      separable from, or merely link (or bind by name) to the interfaces of,\\n      the Work and Derivative Works thereof.\\n\\n      \"Contribution\" shall mean any work of authorship, including\\n      the original version of the Work and any modifications or additions\\n      to that Work or Derivative Works thereof, that is intentionally\\n      submitted to Licensor for inclusion in the Work by the copyright owner\\n      or by an individual or Legal Entity authorized to submit on behalf of\\n      the copyright owner. For the purposes of this definition, \"submitted\"\\n      means any form of electronic, verbal, or written communication sent\\n      to the Licensor or its representatives, including but not limited to\\n      communication on electronic mailing lists, source code control systems,\\n      and issue tracking systems that are managed by, or on behalf of, the\\n      Licensor for the purpose of discussing and improving the Work, but\\n      excluding communication that is conspicuously marked or otherwise\\n      designated in writing by the copyright owner as \"Not a Contribution.\"\\n\\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\\n      on behalf of whom a Contribution has been received by Licensor and\\n      subsequently incorporated within the Work.\\n\\n   2. Grant of Copyright License. Subject to the terms and conditions of\\n      this License, each Contributor hereby grants to You a perpetual,\\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\\n      copyright license to reproduce, prepare Derivative Works of,\\n      publicly display, publicly perform, sublicense, and distribute the\\n      Work and such Derivative Works in Source or Object form.\\n\\n   3. Grant of Patent License. Subject to the terms and conditions of\\n      this License, each Contributor hereby grants to You a perpetual,\\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\\n      (except as stated in this section) patent license to make, have made,\\n      use, offer to sell, sell, import, and otherwise transfer the Work,\\n      where such license applies only to those patent claims licensable\\n      by such Contributor that are necessarily infringed by their\\n      Contribution(s) alone or by combination of their Contribution(s)\\n      with the Work to which such Contribution(s) was submitted. If You\\n      institute patent litigation against any entity (including a\\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\\n      or a Contribution incorporated within the Work constitutes direct\\n      or contributory patent infringement, then any patent licenses\\n      granted to You under this License for that Work shall terminate\\n      as of the date such litigation is filed.\\n\\n   4. Redistribution. You may reproduce and distribute copies of the\\n      Work or Derivative Works thereof in any medium, with or without\\n      modifications, and in Source or Object form, provided that You\\n      meet the following conditions:\\n\\n      (a) You must give any other recipients of the Work or\\n          Derivative Works a copy of this License; and\\n\\n      (b) You must cause any modified files to carry prominent notices\\n          stating that You changed the files; and\\n\\n      (c) You must retain, in the Source form of any Derivative Works\\n          that You distribute, all copyright, patent, trademark, and\\n          attribution notices from the Source form of the Work,\\n          excluding those notices that do not pertain to any part of\\n          the Derivative Works; and\\n\\n      (d) If the Work includes a \"NOTICE\" text file as part of its\\n          distribution, then any Derivative Works that You distribute must\\n          include a readable copy of the attribution notices contained\\n          within such NOTICE file, excluding those notices that do not\\n          pertain to any part of the Derivative Works, in at least one\\n          of the following places: within a NOTICE text file distributed\\n          as part of the Derivative Works; within the Source form or\\n          documentation, if provided along with the Derivative Works; or,\\n          within a display generated by the Derivative Works, if and\\n          wherever such third-party notices normally appear. The contents\\n          of the NOTICE file are for informational purposes only and\\n          do not modify the License. You may add Your own attribution\\n          notices within Derivative Works that You distribute, alongside\\n          or as an addendum to the NOTICE text from the Work, provided\\n          that such additional attribution notices cannot be construed\\n          as modifying the License.\\n\\n      You may add Your own copyright statement to Your modifications and\\n      may provide additional or different license terms and conditions\\n      for use, reproduction, or distribution of Your modifications, or\\n      for any such Derivative Works as a whole, provided Your use,\\n      reproduction, and distribution of the Work otherwise complies with\\n      the conditions stated in this License.\\n\\n   5. Submission of Contributions. Unless You explicitly state otherwise,\\n      any Contribution intentionally submitted for inclusion in the Work\\n      by You to the Licensor shall be under the terms and conditions of\\n      this License, without any additional terms or conditions.\\n      Notwithstanding the above, nothing herein shall supersede or modify\\n      the terms of any separate license agreement you may have executed\\n      with Licensor regarding such Contributions.\\n\\n   6. Trademarks. This License does not grant permission to use the trade\\n      names, trademarks, service marks, or product names of the Licensor,\\n      except as required for reasonable and customary use in describing the\\n      origin of the Work and reproducing the content of the NOTICE file.\\n\\n   7. Disclaimer of Warranty. Unless required by applicable law or\\n      agreed to in writing, Licensor provides the Work (and each\\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\\n      implied, including, without limitation, any warranties or conditions\\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\\n      PARTICULAR PURPOSE. You are solely responsible for determining the\\n      appropriateness of using or redistributing the Work and assume any\\n      risks associated with Your exercise of permissions under this License.\\n\\n   8. Limitation of Liability. In no event and under no legal theory,\\n      whether in tort (including negligence), contract, or otherwise,\\n      unless required by applicable law (such as deliberate and grossly\\n      negligent acts) or agreed to in writing, shall any Contributor be\\n      liable to You for damages, including any direct, indirect, special,\\n      incidental, or consequential damages of any character arising as a\\n      result of this License or out of the use or inability to use the\\n      Work (including but not limited to damages for loss of goodwill,\\n      work stoppage, computer failure or malfunction, or any and all\\n      other commercial damages or losses), even if such Contributor\\n      has been advised of the possibility of such damages.\\n\\n   9. Accepting Warranty or Additional Liability. While redistributing\\n      the Work or Derivative Works thereof, You may choose to offer,\\n      and charge a fee for, acceptance of support, warranty, indemnity,\\n      or other liability obligations and/or rights consistent with this\\n      License. However, in accepting such obligations, You may act only\\n      on Your own behalf and on Your sole responsibility, not on behalf\\n      of any other Contributor, and only if You agree to indemnify,\\n      defend, and hold each Contributor harmless for any liability\\n      incurred by, or claims asserted against, such Contributor by reason\\n      of your accepting any such warranty or additional liability.\\n\\n   END OF TERMS AND CONDITIONS\\n\\n   APPENDIX: How to apply the Apache License to your work.\\n\\n      To apply the Apache License to your work, attach the following\\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\\n      replaced with your own identifying information. (Don\\'t include\\n      the brackets!)  The text should be enclosed in the appropriate\\n      comment syntax for the file format. We also recommend that a\\n      file or class name and description of purpose be included on the\\n      same \"printed page\" as the copyright notice for easier\\n      identification within third-party archives.\\n\\n   Copyright [yyyy] [name of copyright owner]\\n\\n   Licensed under the Apache License, Version 2.0 (the \"License\");\\n   you may not use this file except in compliance with the License.\\n   You may obtain a copy of the License at\\n\\n       http://www.apache.org/licenses/LICENSE-2.0\\n\\n   Unless required by applicable law or agreed to in writing, software\\n   distributed under the License is distributed on an \"AS IS\" BASIS,\\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n   See the License for the specific language governing permissions and\\n   limitations under the License.',\n",
       " 'modelfile': '# Modelfile generated by \"ollama show\"\\n# To build a new Modelfile based on this one, replace the FROM line with:\\n# FROM mistral:latest\\n\\nFROM C:\\\\Users\\\\eduar\\\\.ollama\\\\models\\\\blobs\\\\sha256-e8a35b5937a5e6d5c35d1f2a15f161e07eefe5e5bb0a3cdd42998ee79b057730\\nTEMPLATE \"\"\"[INST] {{ .System }} {{ .Prompt }} [/INST]\"\"\"\\nPARAMETER stop \"[INST]\"\\nPARAMETER stop \"[/INST]\"',\n",
       " 'parameters': 'stop                           \"[INST]\"\\nstop                           \"[/INST]\"',\n",
       " 'template': '[INST] {{ .System }} {{ .Prompt }} [/INST]',\n",
       " 'details': {'parent_model': '',\n",
       "  'format': 'gguf',\n",
       "  'family': 'llama',\n",
       "  'families': ['llama'],\n",
       "  'parameter_size': '7B',\n",
       "  'quantization_level': 'Q4_0'}}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets see how many model are included\n",
    "import ollama\n",
    "#ollama.list()['models']\n",
    "\n",
    "# Pull mistral model\n",
    "ollama.pull(\"mistral\")\n",
    "ollama.show(\"mistral\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stream_name': 'Safety & Operations',\n",
       " 'created_date': '2022-12-28 13:18:45',\n",
       " 'post_id': 6113990,\n",
       " 'comment_id': 5205991,\n",
       " 'like_count': 3,\n",
       " 'report_count': 0,\n",
       " 'username': 'FlaggerForce',\n",
       " 'author_user_id': 'b701ab9f-563a-4425-a389-aff803a8da58',\n",
       " 'author_position': nan,\n",
       " 'author_status': 'active'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadatas[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "An instance of Chroma already exists for C:\\Users\\eduar\\Documents\\Master_Thesis\\GenAI_Thesis_Beekeeper\\data\\datasets_db with different settings",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_text_splitters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CharacterTextSplitter\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_chroma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Chroma\n\u001b[1;32m----> 6\u001b[0m db \u001b[38;5;241m=\u001b[39m \u001b[43mChroma\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPERSIST_DIRECTORY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m retriever \u001b[38;5;241m=\u001b[39m db\u001b[38;5;241m.\u001b[39mas_retriever()\n",
      "File \u001b[1;32mc:\\Users\\eduar\\anaconda3\\envs\\ollama\\lib\\site-packages\\langchain_chroma\\vectorstores.py:158\u001b[0m, in \u001b[0;36mChroma.__init__\u001b[1;34m(self, collection_name, embedding_function, persist_directory, client_settings, collection_metadata, client, relevance_score_fn)\u001b[0m\n\u001b[0;32m    156\u001b[0m         _client_settings \u001b[38;5;241m=\u001b[39m chromadb\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mSettings()\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client_settings \u001b[38;5;241m=\u001b[39m _client_settings\n\u001b[1;32m--> 158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client \u001b[38;5;241m=\u001b[39m \u001b[43mchromadb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_client_settings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_persist_directory \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    160\u001b[0m         _client_settings\u001b[38;5;241m.\u001b[39mpersist_directory \u001b[38;5;129;01mor\u001b[39;00m persist_directory\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_function \u001b[38;5;241m=\u001b[39m embedding_function\n",
      "File \u001b[1;32mc:\\Users\\eduar\\anaconda3\\envs\\ollama\\lib\\site-packages\\chromadb\\__init__.py:274\u001b[0m, in \u001b[0;36mClient\u001b[1;34m(settings, tenant, database)\u001b[0m\n\u001b[0;32m    271\u001b[0m tenant \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(tenant)\n\u001b[0;32m    272\u001b[0m database \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(database)\n\u001b[1;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mClientCreator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtenant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msettings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msettings\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\eduar\\anaconda3\\envs\\ollama\\lib\\site-packages\\chromadb\\api\\client.py:139\u001b[0m, in \u001b[0;36mClient.__init__\u001b[1;34m(self, tenant, database, settings)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    135\u001b[0m     tenant: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m DEFAULT_TENANT,\n\u001b[0;32m    136\u001b[0m     database: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m DEFAULT_DATABASE,\n\u001b[0;32m    137\u001b[0m     settings: Settings \u001b[38;5;241m=\u001b[39m Settings(),\n\u001b[0;32m    138\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 139\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msettings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msettings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtenant \u001b[38;5;241m=\u001b[39m tenant\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatabase \u001b[38;5;241m=\u001b[39m database\n",
      "File \u001b[1;32mc:\\Users\\eduar\\anaconda3\\envs\\ollama\\lib\\site-packages\\chromadb\\api\\client.py:43\u001b[0m, in \u001b[0;36mSharedSystemClient.__init__\u001b[1;34m(self, settings)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     40\u001b[0m     settings: Settings \u001b[38;5;241m=\u001b[39m Settings(),\n\u001b[0;32m     41\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_identifier \u001b[38;5;241m=\u001b[39m SharedSystemClient\u001b[38;5;241m.\u001b[39m_get_identifier_from_settings(settings)\n\u001b[1;32m---> 43\u001b[0m     \u001b[43mSharedSystemClient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_system_if_not_exists\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_identifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msettings\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\eduar\\anaconda3\\envs\\ollama\\lib\\site-packages\\chromadb\\api\\client.py:62\u001b[0m, in \u001b[0;36mSharedSystemClient._create_system_if_not_exists\u001b[1;34m(cls, identifier, settings)\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;66;03m# For now, the settings must match\u001b[39;00m\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m previous_system\u001b[38;5;241m.\u001b[39msettings \u001b[38;5;241m!=\u001b[39m settings:\n\u001b[1;32m---> 62\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     63\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn instance of Chroma already exists for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midentifier\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with different settings\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     64\u001b[0m         )\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_identifer_to_system[identifier]\n",
      "\u001b[1;31mValueError\u001b[0m: An instance of Chroma already exists for C:\\Users\\eduar\\Documents\\Master_Thesis\\GenAI_Thesis_Beekeeper\\data\\datasets_db with different settings"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "#from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "db = Chroma(persist_directory=PERSIST_DIRECTORY)\n",
    "persistent_client = chromadb.PersistentClient()\n",
    "\n",
    "\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Ollama embeddings and vector store\n",
    "embeddings = OllamaEmbeddings(model=\"mistral\")\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)\n",
    "\n",
    "# Create the retriever\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Define the Ollama LLM function\n",
    "def ollama_llm(question, context):\n",
    "    formatted_prompt = f\"Question: {question}\\n\\nContext: {context}\"\n",
    "    response = ollama.chat(model='mistral', messages=[{'role': 'user', 'content': formatted_prompt}])\n",
    "    return response['message']['content']\n",
    "\n",
    "# Define the RAG chain\n",
    "def rag_chain(question):\n",
    "    retrieved_docs = retriever.invoke(question)\n",
    "    formatted_context = format_docs(retrieved_docs)\n",
    "    return ollama_llm(question, formatted_context)\n",
    "\n",
    "# Use the RAG chain\n",
    "result = rag_chain(\"What is Flagger Force?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Ollama embeddings and vector store\n",
    "embeddings = OllamaEmbeddings(model=\"mistral\")\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)\n",
    "\n",
    "# Create the retriever\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Define the Ollama LLM function\n",
    "def ollama_llm(question, context):\n",
    "    formatted_prompt = f\"Question: {question}\\n\\nContext: {context}\"\n",
    "    response = ollama.chat(model='mistral', messages=[{'role': 'user', 'content': formatted_prompt}])\n",
    "    return response['message']['content']\n",
    "\n",
    "# Define the RAG chain\n",
    "def rag_chain(question):\n",
    "    retrieved_docs = retriever.invoke(question)\n",
    "    formatted_context = format_docs(retrieved_docs)\n",
    "    return ollama_llm(question, formatted_context)\n",
    "\n",
    "# Use the RAG chain\n",
    "result = rag_chain(\"What is Flagger Force?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the RAG chain\n",
    "result = rag_chain(\"How many trucks has Flagger Force?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pprint\n",
    "\n",
    "url = 'https://flaggerforce.com/blog'\n",
    "html_doc = requests.get(url)\n",
    "pprint.pprint(html_doc.content)\n",
    "\n",
    "soup = BeautifulSoup(html_doc.content, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.select('p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. split the data into chunks\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community import embeddings\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# urls = [\"https://flaggerforce.com/\",\n",
    "#         \"https://flaggerforce.com/blog/\",\n",
    "#         \"https://flaggerforce.com/about-us/\"]\n",
    "\n",
    "urls = [\"https://flaggerforce.com/\"]\n",
    "        \n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=1000, chunk_overlap=100)\n",
    "doc_splits = text_splitter.split_documents(documents=docs_list)\n",
    "doc_splits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "-",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
